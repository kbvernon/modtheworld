# Univariate Description {#univariate-description}

```{r, include = FALSE, echo = FALSE, child = "R/before_chunk.Rmd"}
```

## Overview

```{r, echo = FALSE, results = 'asis'}

insert_intro(
  goal = "To learn how to model an outcome using properties of the outcome itself.",
  tldr = "For a single variable model, Expectation = central tendency, Error = variability.",
  outcomes = c("measures of central tendency, and", 
               "measures of variability."),
  datasets = list(),
  requirements = list(),
  readings = list()
)

```

As a way of priming you for the topics we'll cover here, consider this somewhat bland adaptation of an already bland scenario. You have six balls, with four of them being red, two gray, as shown in Fig. \@ref(fig:balls-and-urn). You can think of these balls as being the result of an _experiment_, with the outcome of interest being their color. Let's stipulate, as well, that you have no information about _why_ they are these colors. You don't know, for example, that the author has a preference for dark reds and grays.    

```{r balls-and-urn, echo = FALSE, out.width = "50%", fig.cap = "Colored balls and urn. Outcome of an unknown process."}

knitr::include_graphics("images/balls_and_urn.png")

```

As part of this thought experiment, suppose we plop them in a jar, shake it up, then have you select one at random. Without looking, what color do you expect to draw? The answer, I hope, is red. Why? Because there are more of them! So, you are more _likely_ to draw red just by chance. Now, think about what you just did. Lacking any information about how this particular outcome came to be, you still managed to form a pretty reasonable expectation about it solely on the basis of the value it _tends_ toward or is _more likely to take_. This value of the outcome is commonly referred to as its __central tendency__, though you will sometimes hear it described as the _location_ or the _first moment_ of some data. As the name suggests, this is the value around which observations of your outcome tend to cluster.  

Fortunately, that is not the _only_ information we can extract from this scenario, for suppose now that we had you repeat this exercise, say, a hundred times, replacing the ball each time. How often do you think, having guessed red, that gray would actually be the color to come up? Roughly a third of the time, right? And, why is that? Because the proportion of balls that are gray is one-third! That is to say, in spite of your expectation, and just by chance, you should draw a gray ball one-third of the time. Thus, you have won this additional bit of new knowledge, a precise, quantitative expectation about your own error, otherwise known as __variability__, _dispersion_ in the outcome, or your _uncertainty_, also commonly referred to as the _second moment_ of your data. This is a measure not just of the actual but of the _expected_ disagreement between your estimate and the observed outcome.  

To make this a little more tangible and perhaps a little more relevant to you, think about how many hours of sleep you _tend_ to get each night. Whatever it happens to be, this number defines your, shall we say, slumbering expectation. Obviously, this is going to be affected by a lot of variables, like the oceans of coffee you regularly consume, or the way time seems to drag you, despite your protestations, into unavoidable collisions with looming deadlines. Even without that knowledge, though, you can still get a pretty good sense of your sleep _habits_ by looking just at your sleep _tendencies_.  

```{r sleep-pattern, echo = FALSE, fig.height = 4, fig.width = 6.5, fig.cap = "Observed hours of sleep per night over the last thirty days, relative to the expectation of seven hours of sleep per night."}

n <- 30
mu <- 7

xmin <- -0.5
xmax <- n + 1
ymin <- 4
ymax <- 10

set.seed(20210226)

sleep <- rnorm(n, mean = mu)

sleep_df <- data.frame(nights = 1:n, hours = sleep, xend = 1:n, yend = mu)

wee_gray <- "#c7c7c7"

# add text and arrows
arrow_spec <- arrow(length = unit(0.015, "npc"), type = "closed")

labs <- data.frame(x = c(31, 8),
                   y = c(9, 5),
                   xend = c(31, 4.2),
                   yend = c(mu+0.1, 6.5))

ggplot(sleep_df) +
  geom_vline(xintercept = 0, color = "#5c5c5c", size = 0.8) +
  geom_segment(aes(nights, sleep, xend = xend, yend = yend),
               color = "gray", 
               alpha = 0.75) +
  geom_hline(yintercept = mu, color = "darkred", size = 0.8) +
  geom_point(aes(nights, sleep), 
             size = 2.75,
             color = "#5c5c5c") +
  geom_segment(data = labs, aes(x, y, xend = xend, yend = yend),
               arrow = arrow_spec) +
  annotate(geom = "text", 
           x = labs$x[1], 
           y = labs$y[1]+0.45, 
           label = "E[y]", 
           size = 6) +
  annotate(geom = "text", 
           x = labs$x[2]+0.45, 
           y = labs$y[2], 
           label = "epsilon",
           parse = TRUE, 
           size = 6) +
  theme_bw() + 
  theme(legend.position = "none",
        panel.grid = element_blank(),
        panel.border = element_rect(color = wee_gray, fill = NA)) +
  xlim(xmin, xmax) +
  ylim(ymin, ymax) +
  labs(x = "Nights", y = "Hours of sleep")

```

You can even use that expectation to measure variability in the length of "great nature's second course."^[A line from Macbeth I am using because I don't want to say the word 'sleep' five hundred times...] If you know, for example, that you tend to get seven hours of sleep each night, and you conveniently - at least for our purposes - recorded your hours of sleep over the last 30 days, your sleep pattern might look like that shown in Fig \@ref(fig:sleep-pattern). From these observations, you can see that your un-waking life tends to last between five and nine hours (nine blessed hours!) each night. This then allows you to formulate an expectation about your error, that it tends to be two hours more or two hours less than your typical night of sleep. We can actually say even more about your expected error, but we're building up to that!   

Just note for now all that you have already gleaned from these simple observations. In particular, you now have the makings of an empirical model. On the one hand, you have your expectation about the outcome, which you have identified with its central tendency. And, on the other hand, you have your expectation of the error, or uncertainty. Because these descriptions involve a single random variable, you have just engaged in what we will refer to as __univariate description__, defined as the building of a model of an outcome in terms of its central tendency and variability. Framed in terms of our fundamental formula

$$ y = E[y] + \epsilon $$
you have just specified that   

* $E[y] =$ central tendency and  
* $\epsilon \sim$ variability.

Note that both the central tendency and the variability are single values (scalars in R-speak). Specifically, they are _statistics_ that summarize properties of your sample, in this case the value the sample tends to take and the error around that value. Clearly, these are some complex ideas, so this is going to take some unpacking.

Before we get to that, however, we need to emphasize two important limitations of these very simple univariate models, both owing to the fact that the models describe a _sample_ consisting of a _single variable_. First, because they describe a sample, they can only _approximate_ the true central tendency and variability of the _population_. And second, because the sample consists of a single variable, the models cannot in any meaningful sense _explain_ variability in the outcome. Why, you might wonder, did I sleep two more hours than expected that one Monday last month? Answering with "because that's what I did," while technically true, is not terribly informative.  

That being said, univariate models offer us some powerful tools for getting a handle on our data. They'll also play an important role in how we evaluate more complex multivariate models later on, where the goal really is to explain variability in a population, so let's dive into just how exactly we generate these descriptions.  

```{block2, type = "rmdwarning"}
Instead of using $y$ as our variable, we are going to switch to using $x$, as it is common when covering these topics to use that notation. However, nothing hangs on this, and if it helps, you can simply imagine that all the $x$'s are actually $y$'s.
```


## Central tendency

The two most common measures of central tendency are the mean and the median. Their slightly less popular cousin is the mode, which is typically reserved for categorical data. 

### Mean

If you recall, we are defining our expectation for an outcome, $x$, in terms of its central tendency. In this case, we measure the central tendency using the mean, hence our expectation is: 

$$ E[x] = \mu $$

where $\mu$ is the __population mean__. Given the practical limitations of ever measuring this value directly, we must estimate it using the __sample mean__, by convention denoted $\bar{x}$ and defined as the sum of the sample values divided by the number of values (the average, in other words). As a simple formula, that is:

$$ \bar{x} = \frac{1}{n} \sum_{i=1}^{n} x_{i} $$

where $n$ is the number of values and each $x_{i}$ is a value of an individual $i$ in the sample. 

```{block2, type = "rmdnote"}
The sample mean, $\bar{x}$, and the population mean, $\mu$, are calculated using the same function - it's just the average after all, but with slightly different notation. Where $\bar{x}$ uses small $n$ to denote the total number of individuals in a _sample_, $\mu$ uses big $N$ to denote the total number of individuals in a _population_.
```

Suppose we did a study of the sleep behavior of college students on US campuses, using a phone app to measure various aspects of their sleep through a normal semester. To keep this simple, we'll assume a sample size of ten. And the primary outcome we are measuring is hours of sleep per night.  

```{r, echo = FALSE}

set.seed(20210228)

population <- rnorm(n = 1000, mean = 7, sd = 1.3)

population <- round(population, digits = 2)

```

```{r}

x <- sample(population, size = 10)

x

```

Using our simple equation, we can calculate the mean of this sample using the following R code:

```{r}

# number of observations in the sample
n <- length(x)

# sum of the observations
Ex <- sum(x)

# mean of the sample
(1/n) * Ex

```

We can also write that more compactly using the `mean()` function.  

```{r}

mean(x)

```

This sample mean is our estimate of the population mean, it is the number of hours of sleep we expect a _typical_ college student to get. Of course, other details about a student might inform on whether they get more or less sleep than expected. You don't have to look all that hard for the usual suspects:  

* diet: are they, perhaps, subsisting on cheetos and soda?  
* exercise: or lack thereof? 
* stress: are they working two jobs? late on rent? facing important deadlines? 
* disease: are they sick or otherwise unwell?  

These are all critical factors that will affect the amount of sleep an individual is getting. I'm sure you can add to this list with a little thought. Unfortunately, our study - our overly simplistic study - didn't give us that information! All we know is the amount of sleep per night reported in our sample, so that is the only basis we have for forming any expectation about the amount of sleep a random college student should get on any given night. But, don't let that get you down! Even if we had those other variables available and wanted to use them to explain some of that variation around the mean, we would still, obviously, need the sample mean to do it!   

Note that you will on occasion encounter variables that include missing or `NA` values. Because we do not know those values, we cannot _technically_ know the mean of the variable. The function `mean()` registers this fact by returning `NA` in such cases. To avoid this, and to calculate the mean of the _known_ values of the variable, we can specify the parameter `na.rm = TRUE`, which means "remove the NA values before computing the value of the variable." 

```{r, echo = FALSE}

x_NA <- c(x[c(1, 3)], NA, x[c(7, 9, 4, 5)], NA, NA, x[c(2, 6, 8, 10)], NA)

```

```{r}

x_NA

mean(x_NA)

mean(x_NA, na.rm = TRUE)

```

```{r, eval = FALSE, echo = FALSE, fig.width = 2, fig.height = 3.3}

new_df <- data.frame(hours = x, xn = 0)

ggplot(new_df) +
  geom_point(aes(xn, hours), 
             size = 2.75,
             color = "#5c5c5c") +
  annotate(geom = "segment",
           x = -0.25,
           y = mean(x),
           xend = 0.25,
           yend = mean(x),
           color = "darkred", 
           size = 0.8) +
  annotate(geom = "text", 
           x = 0.35, 
           y = mean(x), 
           label = expression(bar(x)), 
           size = 6) +
  theme_bw() + 
  theme(legend.position = "none",
        panel.grid = element_blank(),
        panel.border = element_rect(color = wee_gray, fill = NA),
        axis.ticks.x = element_blank(),
        axis.text.x = element_blank()) +
  xlim(-0.5, 0.5) +
  ylim(ymin, ymax) +
  labs(x = "", y = "Hours of sleep")

```

You will sometimes hear the mean referred to as the _center of mass_ for a set of data because it divides that data into two sets of equal mass. You can thus visualize it as a fulcrum, like on a playground see-saw (or any other lever, really), with your observations piled up on either side so that they are perfectly balanced. Thinking about it this way makes it easier to see how the mean can be sensitive to outliers. That's because as one or more values get bigger and bigger (or smaller and smaller), you have to shift more and more of your observations to one side to counteract the mass of those outliers. Take, for example, income inequality in the United States, something we seem to excel at as a country, for better or worse - just worse, really. The center of mass of US income in 2018 was right around $90,000 a year. It will perhaps come as no surprise to you that most Americans, more than half, made way, way, way less than that. Or, to put that in slightly different terms, a tenth of all income flowed to the top one percent, and nearly half of all income went to the top twenty percent! Meaning, 20% of the populace brings home an income (has a mass) relatively equal to the other 80%! The consequence, of course, is that using the mean will severely over-estimate the real income of a substantial portion of the population.   

We, therefore, need another way of looking at the central tendency of a variable like income that is less sensitive to these outliers. So, let's talk about the concept of a median!  


### Median

The median is the _middle_ value of a variable, or the point that divides its values in half, meaning half the observations are less than this value and half are greater than it. Because it divides the data in this way, finding it requires ordering values of a variable from least to greatest, then looking for the halfway point, that _position_ in the data, and then identifying the value of the variable _there_. The median is that value. This is slightly complicated by the fact that an exact solution will depend on whether a variable contains an even or an odd number of observations, for when the variable has an even number of observations, the median is not the value of an observation but the value halfway between two observations. 

$$ 
med(x) =
\begin{cases}
  x\left[i = \frac{n+1}{2}\right] & \text{if n is odd} \\\\
  \frac{1}{2}\left(x\left[i = \frac{n}{2}\right] + x\left[i = \frac{n}{2}+1\right]\right) & \text{if n is even} 
\end{cases} 
$$

This set of equations might be a little intimidating for the uninitiated, but it actually uses some syntax we learned for indexing vectors in R, specifically the bracket operator, `[`, with $x$ being the vector that is subset: 

```{r, eval = FALSE}
x[i]
```

Thinking about it this way, the procedure really just boils down to two steps: first, find the position $i$ in your data; then, use $i$ to index $x$. If the data is even, of course, you'll have to find two positions, get the values there, then find the value halfway between them. Let's walk through an example with our college sleep data. First, note that it has an even number of observations, specifically ten, so for demonstration purposes we'll need to remove one value to get an odd number.  

```{r}

x_even <- x

x_odd <- x[-length(x)] # remove the last number

```

Now, let's look at our two scenarios.  

__Odd number of observations__

```{r}

# sort values from lowest to highest
x_odd <- sort(x_odd)

x_odd

# count the number of values
n <- length(x_odd)

# find the middle position
i <- (n + 1)/2

# now retrieve the value at that position
x_odd[i]

```

Voila! The median! Simple, right? Well, actually, it's even simpler than you thought, for there's an R function for that.  

```{r}

median(x_odd)

```

__Even number of observations__

```{r}

x_even <- sort(x_even)

n <- length(x_even)

i_left <- n/2
i_right <- i_left + 1

# now we find the value half way between 
# the center left and center right positions
(x_even[i_left] + x_even[i_right])/2

# but again, simpler this way
median(x_even)

```


### Mode

The mode is a measure of the most frequent value in a data set. Of all the measures of central tendency, it is unique in that it can be applied to categorical data. For example, the mode of the colored balls in Fig. \@ref(fig:balls-and-urn) is _red_, as that is the value that occurs most often. On occasion, the mode may also apply to a discrete variable. For instance, the mode of the vector `c(2, 3, 3, 4, 5)` is 3, since it occurs twice and the other values only once. Technically, the mode can be used to describe continuous variables, too, though how often do you expect a number like 42.291092847123452667 to occur in your data? That is to say, for continuous data it is highly unlikely that any two values will ever be _exactly_ the same; hence, it is also unlikely for such data to have a mode.

Unfortunately, R does not have a native function for computing the mode, but we can get around that with some other tools R does offer.^[[https://stackoverflow.com/questions/2547402/how-to-find-the-statistical-mode](https://stackoverflow.com/questions/2547402/how-to-find-the-statistical-mode)] As an example, let's say we're working with a vector of color values for our balls from above.

```{r}

balls <- c("red", "red", "red", "red", "gray", "gray")

# get the unique values
ball_colors <- unique(balls)

# count the number of each
counts <- tabulate(match(balls, ball_colors))

# pick color with largest count
ball_colors[counts == max(counts)]

```

Don't worry, there's a - I was going to say snowball's chance in Texas, but... - anyway, you probably won't ever have to do this. It's solely for demonstration.  


## Variability

```{r expected-error, eval = FALSE, echo = FALSE, fig.height = 4, fig.width = 6.5, fig.cap = "Expected error in hours of sleep per night for college students."}

probs <- data.frame(xmin = rep(-10, 4),
                    xmax = rep(40, 4),
                    ymin = c(5.0, 5.5, 6.0, 6.5), 
                    ymax = c(9.0, 8.5, 8.0, 7.5),
                    fill = LETTERS[1:4])

ggplot() +
  geom_rect(data = probs,
            aes(xmin = xmin,
                xmax = xmax,
                ymin = ymin,
                ymax = ymax,
                fill = fill),
              alpha = 0.35) +
  scale_fill_viridis(option = "inferno", 
                     begin = 0.3,
                     discrete = TRUE, 
                     direction = -1) +
  geom_vline(xintercept = 0, color = "#5c5c5c", size = 0.8) +
  geom_segment(data = sleep_df,
               aes(nights, sleep, xend = xend, yend = yend),
               color = "gray", 
               alpha = 0.75) +
  geom_hline(yintercept = mu, color = "darkred", size = 0.8) +
  geom_point(data = sleep_df,
             aes(nights, sleep), 
             size = 2.75,
             color = "black") +
  theme_bw() + 
  theme(legend.position = "none",
        panel.grid = element_blank(),
        panel.border = element_rect(color = wee_gray, fill = NA)) +
  coord_cartesian(xlim = c(xmin, xmax), ylim = c(ymin, ymax)) +
  labs(x = "Nights", y = "Hours of sleep")

```

Variability, otherwise known as dispersion, uncertainty, or the second moment of a variable, refers to the expected error of a variable. But, what exactly does "expected error" mean? The error is easy enough to explain. As we learned already, it's the difference between the actual values and the expected values:

$$ \epsilon = y - E[y] $$
```{block2, type = 'rmdwarning'}
__Errors vs Residuals__. Strictly speaking, you should refer to $\epsilon$ as the __error__ term _only when_ the expectation is defined by the population mean, $E[y]=\mu$. When the expectation is defined by the sample mean, $E[y]=\bar{x}$, the differences are referred to as __residuals__.
```

Here is how you calculate the residuals in our college sleep data:

```{r}

# get sample mean
xbar <- mean(x)

# calculate errors (epsilon, e)
residuals <- (x - xbar)

```

Now, what about the _expectation_ of the errors? We already have the idea of expected value, which we identified with the central tendency, typically the mean of the observations. As a first approximation then, we might try to measure variability using the mean of the errors: 

$$ \frac{1}{n} \sum_{i=1}^{n} (x - \bar{x}) $$

In R, that's:

```{r, render = function(x, options) {print(round(x, digits = 3))}}

# number of residuals
n_residuals <- length(residuals)

# calculate mean of residuals
sum(residuals) / n_residuals

```

It's zero! But, why? Let's look at the way the actual values fall out relative to the mean, as shown in Fig. \@ref(fig:center-mass).  

```{r center-mass, echo = FALSE, fig.width = 3.3, fig.height = 2, fig.cap = "Center of mass and residuals of college sleep data."}

set.seed(20210228)

yn <- runif(length(x), -0.15, 0.15)

xbar <- mean(x)

new_df <- data.frame(hours = x, 
                     yn = yn,
                     xend = xbar)

ggplot(new_df) +
  geom_segment(aes(hours, yn, xend = xend, yend = yn),
               color = "gray", 
               alpha = 0.75) +
  geom_point(aes(hours, yn), 
             size = 2.75,
             color = "#5c5c5c") +
  annotate(geom = "segment",
           x = xbar,
           y = -0.25,
           xend = xbar,
           yend = 0.25,
           color = "darkred", 
           size = 0.8) +
  annotate(geom = "text", 
           x = xbar, 
           y = 0.35, 
           label = "paste(bar(x))", 
           size = 4,
           parse = TRUE) +
  theme_bw() + 
  theme(legend.position = "none",
        panel.grid = element_blank(),
        panel.border = element_rect(color = wee_gray, fill = NA),
        axis.ticks.y = element_blank(),
        axis.text.y = element_blank()) +
  xlim(ymin, ymax) +
  ylim(-0.4, 0.4) +
  labs(x = "Hours of sleep", y = "")

```

Now, let's compare the sum of the residuals above the mean (residuals greater than zero) and the sum of the residuals below the mean (residuals less than zero):

```{r}

sum(residuals[residuals < 0])

sum(residuals[residuals > 0])

```

They're the same, just with different signs. That means when you sum all of them together, you get zero! Hence, dividing by the number of observations, $n$, still returns zero as the mean of the residuals. How do we fix this? Consider the fact that what we want is not just the _difference_ between the observed and the expected but the _distance_ between them, which is always positive. Now, how do we get that? The standard solution is to square the residuals before dividing by the number of observations. This is known as the __variance__.  


### Variance

To be precise, variance is defined as the mean squared difference between the observed and the expected values. For discrete variables, the population variance, $\sigma^{2}$, is given by:

$$ \sigma^{2} = \frac{1}{N} \sum_{i=1}^{N} (x_{i}-\mu)^{2} $$
The sample variance, $s^{2}$, is then defined as:

$$ s^{2} = \frac{1}{n-1} \sum_{i=1}^{n} (x_{i} - \bar{x})^{2} $$
Notice the denominator is now $n-1$, not just $n$. Subtracting one from $n$ is known as Bessel's correction. It is a way of addressing the fact that the sample variance tends to underestimate the population variance. When it is included, the resulting statistic is referred to as the _unbiased sample variance_.  

Let's compute this statistic for our college sleep sample.

```{r, render = function(x, options) {print(round(x, digits = 3))}}

sum(residuals^2) / (n_residuals - 1)

```

As with the other statistics we calculated, you do not have to do this all manually in R. Instead, you can use the `var()` function.

```{r, render = function(x, options) {print(round(x, digits = 3))}}

var(x)

```

But, what exactly is this value? If you recall, the variable we are describing is _hours per night_ of sleep. Since we squared the difference between the observed and expected values of that variable, the variance is now in units of _squared hours per night_, which is - like what even is that really? It is hard to interpret, and harder still to mesh with our original data. To address this issue, it is common to take the square root of the variance. The resulting value is known as the standard deviation.


### Standard deviation

The population standard deviation, $\sigma$, is the square root of the population variance, $\sigma^{2}$, and the sample standard deviation, $s$, is the square root of the unbiased sample variance, $s^{2}$. Formally,

$$
\sigma = \sqrt{\sigma^{2}} \\
s = \sqrt{s^{2}}
$$
Again, the sample standard deviation is only an estimate of the population standard deviation. Here is the calculation by hand and with the base R `sd()` function.  

```{r, render = function(x, options) {print(round(x, digits = 3))}}

sqrt(var(x))

sd(x)

```

